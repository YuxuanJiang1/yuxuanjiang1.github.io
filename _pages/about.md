---
layout: about
width: 1500px
title: About
permalink: /
subtitle: >
  <b>PhD Student</b> ⎟ <b>LLMs & Tool-Integrated Reasoning</b> ⎟ <b>NLP, LLM Reasoning</b>

profile:
  align: right
  image: prof_pic.jpg
  more_info: >
    <h4 class="mb-0">Yuxuan Jiang</h4>
    <p class="mb-1"><span class="emph">yuxuanj1</span>@<span class="emph">umbc</span>.<span class="emph">edu</span></p>

news: true
latest_posts: false
selected_papers: true
social: true
announcement: >
  I am currently open to <b>research collaborations</b> in large language models reasoning.  
  Please <a href="mailto:yuxuanj1@umbc.edu" target="_blank">email me</a> if you’d like to discuss a project.

blog_newsletter: false
notes_newsletter: true
---

Yuxuan Jiang is a Computer Science PhD student under [Dr. Francis Ferraro](https://www.csee.umbc.edu/people/faculty/frank-ferraro/) at the University of Maryland, Baltimore County (UMBC).  
His research centers on **tool-integrated reasoning**, **LLM Memorization**, and **efficient large language models**.

He explores how structured reasoning patterns—what he calls *Tool Cards*—can improve LLMs’ ability to solve math and logic problems efficiently.  
His ongoing projects include building large-scale reasoning benchmarks, designing template-guided fine-tuning pipelines, and studying how reasoning efficiency relates to model memory and inference balance.

Previously, Yuxuan earned his M.S. in Computer Science from **New York University** and B.A. in Information Systems from **Beijing Language and Culture University**.  
He also interned at **Xiaomi**, where he worked on large-scale dialogue systems and commonsense reasoning integration, and at the **National Language Resource Monitoring Center**, where he led semantic role labeling and input-correction research.

He has authored several papers on large language model reasoning and evaluation, including works on memorization mitigation and reasoning-step compression.

{% include announcement.html %}

Yuxuan has a strong background in **deep learning**, **natural language processing**, with experience in PyTorch, Hugging Face, TensorFlow, and cloud deployment (AWS / GCP).  
He has also been recognized for his contributions to research on *reasoning efficiency*, *skill-aware step pruning*, and *distilled reasoning models* in recent ACL and EMNLP submissions.

## Research Interests

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;✓ Tool-Integrated Reasoning  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;✓ Large Language Models Reasoning 

